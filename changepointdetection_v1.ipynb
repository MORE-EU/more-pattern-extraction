{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6c13b0",
   "metadata": {},
   "source": [
    "# Changepoint detection\n",
    "In this notebook, we present a tool for detecting and recognizing changepoints in a time series. Given a set of intervals in the time series, where changepoints may lie, our tool essentially ranks those intervals with respect to the possibility of containing a changepoint.\n",
    "\n",
    "We focus on a particular use case, where each point in the input time series contains measurements for power production and various environmental parameters in a solar panel park, and we are looking for changepoints among all sufficiently heavy rains. Detecting a changepoint when a rain occurred is considered an indication that  soiling affected productivity of the park before the rain, but the rain lead to improved productivity through washing off dirt from the panels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f4147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "plt.rcParams['savefig.facecolor']='w'\n",
    "%matplotlib inline \n",
    "from my_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ee8b6",
   "metadata": {},
   "source": [
    "# Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1910ad61-2a60-44b8-9a9e-611e5b23920b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../soiling_data/PV 1-20190101T000000/PV 1-20190101T000000_msrc15m.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79144/1655784579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_strings2019\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../soiling_data/PV 1-20190101T000000/PV 1-20190101T000000_msrc15m.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../soiling_data/Weather Data-20190101T000000/Weather Data-20190101T000000_msrc15m.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_strings2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_strings2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Panel group output power (kW) [Array 1]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Total Irradiance (W*m^-2) [Array 1]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Performance index (to date) (%) [Array 1]'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'Module Temperature (C) [Array 1]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_weather\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#because column labels are shifted while reading the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../soiling_data/PV 1-20190101T000000/PV 1-20190101T000000_msrc15m.csv'"
     ]
    }
   ],
   "source": [
    "file_strings2020 = \"../soiling_data/PV1 -20200101T000000v0/PV1 -20200101T000000_msrc15m.csv\"\n",
    "file_strings2019 = \"../soiling_data/PV 1-20190101T000000/PV 1-20190101T000000_msrc15m.csv\"\n",
    "file_weather = \"../soiling_data/Weather Data-20190101T000000/Weather Data-20190101T000000_msrc15m.csv\"\n",
    "df_strings = pd.concat([pd.read_csv(file_strings2019),pd.read_csv(file_strings2020)])[['Panel group output power (kW) [Array 1]','Total Irradiance (W*m^-2) [Array 1]', 'Performance index (to date) (%) [Array 1]',  'Module Temperature (C) [Array 1]']]\n",
    "df_weather = pd.read_csv(file_weather)\n",
    "df_weather = df_weather.iloc[:, :-1] #because column labels are shifted while reading the file\n",
    "df_weather = df_weather.rename_axis('timestamp')\n",
    "df_weather.index = pd.to_datetime(df_weather.index)\n",
    "df_weather.columns = ['air_temp', 'irradiance', 'module_temp', 'precipitation', 'humidity', 'wind_dir', 'wind_speed']\n",
    "df_weather_clean = df_weather.dropna(subset=['precipitation']).copy()\n",
    "#str_no = 1 # 1-indexed\n",
    "df_strings = df_strings.rename_axis('timestamp')\n",
    "df_strings.columns = [ 'power', 'irradiance', 'perf_index', 'mod_temp']\n",
    "df_strings.index = pd.to_datetime(df_strings.index)\n",
    "start = '11:00'\n",
    "end = '16:00'\n",
    "df_strings = df_strings.between_time(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings_clean = df_strings.dropna()\n",
    "df_strings_clean = df_strings_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b485047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072022bc",
   "metadata": {},
   "source": [
    "# Filter possible changepoints\n",
    "In the specific use case that we explore here, we have a set of time windows corresponding to possible locations where changepoints may lie. These time windows correspond to the rains. In particular, we investigate rains with precipitation that is sufficiently high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3749e-484b-4576-9b72-18db5a08dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = pd.DataFrame(index=df_weather_clean.index)\n",
    "precipitation = pd.concat([pd.Series({'2019-01-01 00:15:00': 0}),df_weather_clean.precipitation])\n",
    "precipitation.index = pd.to_datetime(precipitation.index)\n",
    "print(precipitation.index)\n",
    "df_dates[\"rain_start\"] = precipitation[(precipitation.shift(-1) > 0) & (precipitation == 0)] # compare current to next\n",
    "df_dates[\"rain_stop\"] = precipitation[(precipitation.shift(1) > 0) & (precipitation == 0)] # compare current to prev\n",
    "#print(df_dates[\"rain_start\"])\n",
    "dates_rain_start = df_dates.rain_start.index[df_dates.rain_start.notna()]\n",
    "dates_rain_stop = df_dates.rain_stop.index[df_dates.rain_stop.notna()]\n",
    "dates = dates_rain_start.union(dates_rain_stop)\n",
    "ax = precipitation.plot(figsize=(20,10))\n",
    "for d in dates:\n",
    "    ax.axvline(x=d, color='k', linestyle=':')\n",
    "plt.show()\n",
    "#df_dates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd218b",
   "metadata": {},
   "source": [
    "### Filter out light rains\n",
    "Any rain with maximum precipitation less than 0.8 is considered light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2802ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for idx in range(dates_rain_start.size):\n",
    "    d1 = dates_rain_start[idx]\n",
    "    d2 = dates_rain_stop[idx]\n",
    "    if np.max(precipitation.loc[d1:d2]) >= 0.8:\n",
    "        ids.append(idx)\n",
    "dates_rain_start_filtered = dates_rain_start[ids]\n",
    "dates_rain_stop_filtered = dates_rain_stop[ids]\n",
    "dates_rain_start_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb4ac0-5fca-409d-958c-aa46bdc1c04e",
   "metadata": {},
   "source": [
    "# Detect changepoints\n",
    "We iterate over all possible rains, which correspond to possible time-windows where a changepoint may occur. In each of those locations, we define a time window before the assumed changepoint as our training set, one time-window coming right after the training set but before the time of the assumed changepoint, as our validation set, and one time window after the assumed changepoint, as our test set. The intuition is the following: our training set corresponds to a time window where our targe value remains relatively low. Similar behavior is expected in our validation test. We use the training set to train a regression model, and we use the corresponding validation test to validate.  Then, if a changepoint is indeed located in the assumed time window, the real target value in our test set will deviate significantly from the prediction of our model. In particular, in this specific experiment, we are only interested in sudden increases of the target value (rather than any kind of deviation in the test set).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa239f",
   "metadata": {},
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 15 # window of days to train (before the rain)\n",
    "w2 = 5  # window of days to validate (before the rain)\n",
    "w3 = 5 # window of days to validate (after the rain)\n",
    "training_scores = np.zeros((dates_rain_start_filtered.size, 5))\n",
    "validation_scores = np.zeros((dates_rain_start_filtered.size, 5))\n",
    "test_scores = np.zeros((dates_rain_start_filtered.size, 5))\n",
    "scaler = MinMaxScaler()\n",
    "df_strings_clean = pd.DataFrame(scaler.fit_transform(df_strings_clean), columns=df_strings_clean.columns, index=df_strings_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147bb0d",
   "metadata": {},
   "source": [
    "## For each rain, train, validate and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21ad82-d90f-499d-870a-ae6cacbbbe77",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_val = []\n",
    "preds_test = []\n",
    "preds_train = []\n",
    "for i in range(dates_rain_start_filtered.size):\n",
    "    d1 = dates_rain_start_filtered[i]\n",
    "    d2 = dates_rain_stop_filtered[i]\n",
    "    #changepoint_scores(df_strings_clean, d1, d2, w1, w2, w3)\n",
    "    try:\n",
    "         y_pred_train, score_train, y_pred_val, score_val, y_pred_test, score_test = changepoint_scores(df_strings_clean, d1, d2, w1, w2, w3)\n",
    "         preds_train.append(y_pred_train)\n",
    "         preds_test.append(y_pred_test)\n",
    "         preds_val.append(y_pred_val)\n",
    "         training_scores[i] = score_train\n",
    "         validation_scores[i] = score_val\n",
    "         test_scores[i] = score_test\n",
    "    except:\n",
    "         preds_train.append([])\n",
    "         preds_test.append([])\n",
    "         preds_val.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6094d2",
   "metadata": {},
   "source": [
    "## Plot the results that deviate most from the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7d626-1ad4-4c65-9b5b-aa1f5832e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_column = 3 # 0=r_squared, 1=mae, 2=me, 3=mape, 4=mpe\n",
    "thrsh = np.sort(validation_scores[:,score_column]) # take the top 10 validation scores\n",
    "thrsh = thrsh[10]\n",
    "indices_test = np.where(validation_scores <= thrsh)\n",
    "mask = np.ones(test_scores.shape, dtype=bool)\n",
    "mask[indices_test] = False\n",
    "test_scores[mask] = np.finfo('d').min\n",
    "indices = np.argsort(-test_scores[:, score_column])[:5] \n",
    "for n, i in enumerate(indices):\n",
    "    d1 = dates_rain_start_filtered[i]\n",
    "    d2 = dates_rain_stop_filtered[i]\n",
    "    window_before = pd.to_datetime(d1) - pd.Timedelta(days=w1)-pd.Timedelta(days=w2)\n",
    "    window_after = pd.to_datetime(d2) + pd.Timedelta(days=w3)\n",
    "    \n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.title(f\"Case #{n + 1}: True Power vs Predicted Power after {d2}\")\n",
    "    plt.plot(df_strings_clean.loc[window_before:window_after].power.values, label=\"True Power\")\n",
    "    nobs = df_strings_clean.loc[window_before:window_after].shape[0]\n",
    "    if (np.array(preds_train[i]).shape[0]) == 0:\n",
    "        continue\n",
    "    preds_empty = [np.nan] * (nobs - len(preds_train[i]) - len(preds_val[i])-len(preds_test[i])) \n",
    "    preds = np.hstack([preds_train[i], preds_val[i], preds_empty, preds_test[i]])\n",
    "    plt.plot(preds, label=\"Expected Power (regression)\")\n",
    "    plt.axvline(x=preds_train[i].shape[0] - 1, color='b', linestyle=':', label=f'Validation start ')\n",
    "    plt.axvline(x=preds_train[i].shape[0]+preds_val[i].shape[0] - 1, color='g', linestyle=':', label=f'Validation end - Rain start {d1}')\n",
    "    plt.axvline(x=preds_train[i].shape[0]+preds_val[i].shape[0] + len(preds_empty), color='r', linestyle=':', label=f'Rain end {d2}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('Metrics before the rain (training_set):')\n",
    "    print(f'MAE:{training_scores[i, 1]:.3f}  \\nME(true-pred):{training_scores[i,2]:.3f} \\nMAPE:{training_scores[i, 3]:.3f} \\nR2: {-training_scores[i, 0]:.3f}\\n')\n",
    "    print('Metrics before the rain (validation_set):')\n",
    "    print(f'MAE:{validation_scores[i, 1]:.3f}  \\nME(true-pred):{validation_scores[i,2]:.3f} \\nMAPE:{validation_scores[i, 3]:.3f} \\nMPE:{validation_scores[i, 4]:.3f} \\nR2: {-validation_scores[i, 0]:.3f}\\n')\n",
    "    print('Metrics after the rain (test_set):')\n",
    "    print(f'MAE:{test_scores[i, 1]:.3f}  \\nME(true-pred):{test_scores[i,2]:.3f}  \\nMAPE:{test_scores[i, 3]:.3f} \\nMPE:{test_scores[i, 4]:.3f} \\nR2: {-test_scores[i, 0]:.3f}')\n",
    "    \n",
    "    precip = df_weather_clean.loc[d1:d2][1:-1].precipitation.values\n",
    "    p_min = precip.min()\n",
    "    p_max = precip.max()\n",
    "    p_mean = precip.mean()\n",
    "    \n",
    "    print('\\nPrecipitation during the rain interval:')\n",
    "    print(f'Mean: {p_mean}, Max: {p_max}, Min: {p_min}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
